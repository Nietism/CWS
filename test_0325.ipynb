{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare_data\n",
    "\n",
    "from Feature import Feature\n",
    "from Decoder import Decoder\n",
    "import operator\n",
    "\n",
    "mode = 'train_avg'\n",
    "iter = 10\n",
    "beam_size = 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original definition of function \"train_avg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_avg(iterations, train_file, beam_size):\n",
    "#     data = prepare_data.read_file(train_file)\n",
    "#     feature = Feature()\n",
    "#     decoder = Decoder(beam_size, feature.get_score)\n",
    "#     n = 0\n",
    "#     for t in range(iterations):\n",
    "#         count = 0\n",
    "#         data_size = len(data)\n",
    "\n",
    "#         for line in data:\n",
    "#             n += 1\n",
    "#             y = line.split()\n",
    "#             z = decoder.beamSearch(line)\n",
    "#             if z != y:\n",
    "#                 feature.update_avgWeight(y, z, n, t, data_size)\n",
    "\n",
    "#             train_seg = ' '.join(z)\n",
    "\n",
    "#             count += 1\n",
    "#             if count % 1000 == 0:\n",
    "#                 print(\"iter %d , finish %.2f%%\" %\n",
    "#                       (t, (count / data_size) * 100))\n",
    "\n",
    "#         model_file = open(\"cv-model_result/model-\" +\n",
    "#                             str(t) + \"_beam-size-\" + str(beam_size) + '.pkl', 'wb')\n",
    "#         feature.save_model(model_file)\n",
    "#         model_file.close()\n",
    "#         print(\"segment with model-%d finish\" % t)\n",
    "#         print(\"iteration %d finish\" % t)\n",
    "\n",
    "#     feature.last_update(iterations, data_size)\n",
    "#     feature.cal_avg_weight(iterations, data_size)\n",
    "#     avg_model = open(\n",
    "#         \"cv-model_result/avg-model_beam-size-\" + str(beam_size) + '.pkl', 'wb')\n",
    "#     feature.save_model(avg_model)\n",
    "#     avg_model.close()\n",
    "#     print(\"segment with avg-model finish\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_file = 'data/filter_train.txt'\n",
    "# test_file = 'data/filter_test.txt'\n",
    "\n",
    "# train_avg(iter, train_file, beam_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### my training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "# import numpy as np\n",
    "# import prepare_data\n",
    "\n",
    "# from Feature import Feature\n",
    "# from Decoder import Decoder\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# train_file = 'data/fusion_data.txt'\n",
    "# raw_data = prepare_data.read_file(train_file)\n",
    "\n",
    "# for train_id, test_id in kf.split(raw_data):\n",
    "#     print(\"TRAIN:\", train_id, \"TEST:\", test_id)\n",
    "#     train_data, test_data = np.array(raw_data)[train_id], np.array(raw_data)[test_id]\n",
    "#     print(len(raw_data), len(train_data), len(test_data))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [    0     1     2 ... 19480 19481 19482] TEST: [    6    17    20 ... 19463 19473 19483]\n",
      "19484 15587 3897\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import prepare_data\n",
    "\n",
    "from Feature import Feature\n",
    "from Decoder import Decoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "train_file = 'data/filter_train.txt'\n",
    "raw_data = prepare_data.read_file(train_file)\n",
    "\n",
    "for train_id, test_id in kf.split(raw_data):\n",
    "    print(\"TRAIN:\", train_id, \"TEST:\", test_id)\n",
    "    train_data, test_data = np.array(raw_data)[train_id], np.array(raw_data)[test_id]\n",
    "    print(len(raw_data), len(train_data), len(test_data))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = 'data/filter_test.txt'\n",
    "# raw_data = prepare_data.read_file(test_file)\n",
    "\n",
    "# sent = raw_data[0]\n",
    "# split_sent = [char for char in sent if(char != ' ') and (char != '\\n')]\n",
    "# split_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_avg(iterations, train_data, beam_size, cv_epoch_num):\n",
    "    # data = prepare_data.read_file(train_file)\n",
    "    data = train_data\n",
    "    feature = Feature()\n",
    "    decoder = Decoder(beam_size, feature.get_score)\n",
    "    n = 0\n",
    "    for t in range(iterations):\n",
    "        count = 0\n",
    "        data_size = len(data)\n",
    "\n",
    "        for line in data:\n",
    "            n += 1\n",
    "            y = line.split()\n",
    "            z = decoder.beamSearch(line)\n",
    "            if z != y:\n",
    "                feature.update_avgWeight(y, z, n, t, data_size)\n",
    "\n",
    "            train_seg = ' '.join(z)\n",
    "\n",
    "            count += 1\n",
    "            if count % 1000 == 0:\n",
    "                print(\"iter %d , finish %.2f%%\" % \\\n",
    "                        (t, (count / data_size) * 100))\n",
    "\n",
    "        model_file = open('cv-' + str(cv_epoch_num) + \"-model_result/model-\" + \\\n",
    "                            str(t) + \"_beam-size-\" + str(beam_size) + '.pkl', 'wb')\n",
    "        feature.save_model(model_file)\n",
    "        model_file.close()\n",
    "        print(\"segment with model-%d finish\" % t)\n",
    "        print(\"iteration %d finish\" % t)\n",
    "\n",
    "    feature.last_update(iterations, data_size)\n",
    "    feature.cal_avg_weight(iterations, data_size)\n",
    "    avg_model = open(\n",
    "        'cv-' + str(cv_epoch_num) + \"-model_result/avg-model_beam-size-\" + str(beam_size) + '.pkl', 'wb')\n",
    "    feature.save_model(avg_model)\n",
    "    avg_model.close()\n",
    "    print(\"segment with avg-model finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_avg(iterations, test_data, beam_size, cv_epoch_num):\n",
    "    # data = prepare_data.read_file(test_file)\n",
    "    data = test_data\n",
    "    feature = Feature()\n",
    "    decoder = Decoder(beam_size, feature.get_score)\n",
    "\n",
    "    count = 0\n",
    "    data_size = len(data)\n",
    "\n",
    "    model_file = open(\n",
    "        'cv-' + str(cv_epoch_num) + '-model_result/avg-model_beam-size-' + str(beam_size) + '.pkl', 'rb')\n",
    "    feature.load_model(model_file)\n",
    "    model_file.close()\n",
    "    for line in data:\n",
    "        z = decoder.beamSearch(line)\n",
    "        seg_data = ' '.join(z)\n",
    "        seg_data_file = 'cv-' + str(cv_epoch_num) + '-test_seg_data/avg-test-seg-data' + \\\n",
    "            '_beam-size-' + str(beam_size) + '.txt'\n",
    "        with open(seg_data_file, 'a') as f:\n",
    "            f.write(seg_data + '\\n')\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(\"segment with avg-model, finish %.2f%%\" %\n",
    "                  ((count / data_size) * 100))\n",
    "    f.close()\n",
    "    print(\"segment with avg model finish\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [    0     1     3 ... 38326 38327 38328] TEST: [    2     8    16 ... 38311 38313 38318]\n",
      "cross-valid epoch: 1\n",
      "iter 0 , finish 3.26%\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "train_file = 'data/fusion_data.txt'\n",
    "raw_data = prepare_data.read_file(train_file)\n",
    "cv_epoch_num = 1\n",
    "\n",
    "for train_id, test_id in kf.split(raw_data):\n",
    "    print(\"TRAIN:\", train_id, \"TEST:\", test_id)\n",
    "    train_data, test_data = np.array(raw_data)[train_id], np.array(raw_data)[test_id]\n",
    "    print(\"cross-valid epoch:\", cv_epoch_num)\n",
    "\n",
    "    train_avg(iterations=10, train_data=train_data, beam_size=16, cv_epoch_num=cv_epoch_num)\n",
    "    test_avg(iterations=10, test_data=test_data, beam_size=16, cv_epoch_num=cv_epoch_num)\n",
    "\n",
    "    cv_epoch_num += 1\n",
    "    # print(len(raw_data), len(train_data), len(test_data))\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6152f7bfa03653fb118ecd7c5fd279534c3a46a58fd7506dc522a094114d8e26"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('torch_cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
