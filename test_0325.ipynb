{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import prepare_data\n",
    "\n",
    "# from Feature import Feature\n",
    "# from Decoder import Decoder\n",
    "# import operator\n",
    "\n",
    "# mode = 'train_avg'\n",
    "# iter = 10\n",
    "# beam_size = 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original definition of function \"train_avg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_avg(iterations, train_file, beam_size):\n",
    "#     data = prepare_data.read_file(train_file)\n",
    "#     feature = Feature()\n",
    "#     decoder = Decoder(beam_size, feature.get_score)\n",
    "#     n = 0\n",
    "#     for t in range(iterations):\n",
    "#         count = 0\n",
    "#         data_size = len(data)\n",
    "\n",
    "#         for line in data:\n",
    "#             n += 1\n",
    "#             y = line.split()\n",
    "#             z = decoder.beamSearch(line)\n",
    "#             if z != y:\n",
    "#                 feature.update_avgWeight(y, z, n, t, data_size)\n",
    "\n",
    "#             train_seg = ' '.join(z)\n",
    "\n",
    "#             count += 1\n",
    "#             if count % 1000 == 0:\n",
    "#                 print(\"iter %d , finish %.2f%%\" %\n",
    "#                       (t, (count / data_size) * 100))\n",
    "\n",
    "#         model_file = open(\"cv-model_result/model-\" +\n",
    "#                             str(t) + \"_beam-size-\" + str(beam_size) + '.pkl', 'wb')\n",
    "#         feature.save_model(model_file)\n",
    "#         model_file.close()\n",
    "#         print(\"segment with model-%d finish\" % t)\n",
    "#         print(\"iteration %d finish\" % t)\n",
    "\n",
    "#     feature.last_update(iterations, data_size)\n",
    "#     feature.cal_avg_weight(iterations, data_size)\n",
    "#     avg_model = open(\n",
    "#         \"cv-model_result/avg-model_beam-size-\" + str(beam_size) + '.pkl', 'wb')\n",
    "#     feature.save_model(avg_model)\n",
    "#     avg_model.close()\n",
    "#     print(\"segment with avg-model finish\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_file = 'data/filter_train.txt'\n",
    "# test_file = 'data/filter_test.txt'\n",
    "\n",
    "# train_avg(iter, train_file, beam_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### my training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "# import numpy as np\n",
    "# import prepare_data\n",
    "\n",
    "# from Feature import Feature\n",
    "# from Decoder import Decoder\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# train_file = 'data/fusion_data.txt'\n",
    "# raw_data = prepare_data.read_file(train_file)\n",
    "\n",
    "# for train_id, test_id in kf.split(raw_data):\n",
    "#     print(\"TRAIN:\", train_id, \"TEST:\", test_id)\n",
    "#     train_data, test_data = np.array(raw_data)[train_id], np.array(raw_data)[test_id]\n",
    "#     print(len(raw_data), len(train_data), len(test_data))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = 'data/filter_test.txt'\n",
    "# raw_data = prepare_data.read_file(test_file)\n",
    "\n",
    "# sent = raw_data[0]\n",
    "# split_sent = [char for char in sent if(char != ' ') and (char != '\\n')]\n",
    "# split_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_avg(iterations, train_data, beam_size, cv_epoch_num):\n",
    "    import logging\n",
    "    # data = prepare_data.read_file(train_file)\n",
    "    data = train_data\n",
    "    feature = Feature()\n",
    "    decoder = Decoder(beam_size, feature.get_score)\n",
    "    n = 0\n",
    "    for t in range(iterations):\n",
    "        count = 0\n",
    "        data_size = len(data)\n",
    "\n",
    "        for line in data:\n",
    "            n += 1\n",
    "            y = line.split()\n",
    "            z = decoder.beamSearch(line)\n",
    "            if z != y:\n",
    "                feature.update_avgWeight(y, z, n, t, data_size)\n",
    "\n",
    "            train_seg = ' '.join(z)\n",
    "\n",
    "            count += 1\n",
    "            if count % 1000 == 0:\n",
    "                print(\"iter %d , finish %.2f%%\" % \\\n",
    "                        (t, (count / data_size) * 100))\n",
    "                logging.info(\"iter %d , finish %.2f%%\" % \\\n",
    "                        (t, (count / data_size) * 100))\n",
    "\n",
    "        model_file = open('cv-' + str(cv_epoch_num) + \"-model_result/model-\" + \\\n",
    "                            str(t) + \"_beam-size-\" + str(beam_size) + '.pkl', 'wb')\n",
    "        feature.save_model(model_file)\n",
    "        model_file.close()\n",
    "        print(\"segment with model-%d finish\" % t)\n",
    "        logging.info(\"segment with model-%d finish\" % t)\n",
    "        print(\"iteration %d finish\" % t)\n",
    "        logging.info(\"iteration %d finish\" % t)\n",
    "\n",
    "    feature.last_update(iterations, data_size)\n",
    "    feature.cal_avg_weight(iterations, data_size)\n",
    "    avg_model = open(\n",
    "        'cv-' + str(cv_epoch_num) + \"-model_result/avg-model_beam-size-\" + str(beam_size) + '.pkl', 'wb')\n",
    "    feature.save_model(avg_model)\n",
    "    avg_model.close()\n",
    "    print(\"segment with avg-model finish\")\n",
    "    logging.info(\"segment with avg-model finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_avg(iterations, test_data, beam_size, cv_epoch_num):\n",
    "    import logging\n",
    "    # data = prepare_data.read_file(test_file)\n",
    "    data = test_data\n",
    "    feature = Feature()\n",
    "    decoder = Decoder(beam_size, feature.get_score)\n",
    "\n",
    "    count = 0\n",
    "    data_size = len(data)\n",
    "\n",
    "    model_file = open(\n",
    "        'cv-' + str(cv_epoch_num) + '-model_result/avg-model_beam-size-' + str(beam_size) + '.pkl', 'rb')\n",
    "    feature.load_model(model_file)\n",
    "    model_file.close()\n",
    "    for line in data:\n",
    "        z = decoder.beamSearch(line)\n",
    "        seg_data = ' '.join(z)\n",
    "        seg_data_file = 'cv-' + str(cv_epoch_num) + '-test_seg_data/avg-test-seg-data' + \\\n",
    "            '_beam-size-' + str(beam_size) + '.txt'\n",
    "        with open(seg_data_file, 'a') as f:\n",
    "            f.write(seg_data + '\\n')\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(\"segment with avg-model, finish %.2f%%\" %\n",
    "                  ((count / data_size) * 100))\n",
    "            logging.info(\"segment with avg-model, finish %.2f%%\" %\n",
    "                  ((count / data_size) * 100))\n",
    "    f.close()\n",
    "    print(\"segment with avg model finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "\n",
    "# logging.basicConfig(filename='5-fold-cv-process.log', \\\n",
    "#     format='%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s', level=logging.DEBUG)\n",
    "# logging.info(\"This is a info log.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross-valid epoch: 1\n",
      "TRAIN: [    1     2     3 ... 38326 38327 38328] TEST: [    0     7     9 ... 38307 38310 38315]\n",
      "file filtering completed\n",
      "len of raw_data: 38329 train_data: 30663 test_data: 7666\n",
      "-------------------------------------------------------------------------------------------------\n",
      "cross-valid epoch: 2\n",
      "TRAIN: [    0     1     2 ... 38326 38327 38328] TEST: [    5    12    13 ... 38320 38321 38324]\n",
      "file filtering completed\n",
      "len of raw_data: 38329 train_data: 30663 test_data: 7666\n",
      "-------------------------------------------------------------------------------------------------\n",
      "cross-valid epoch: 3\n",
      "TRAIN: [    0     2     3 ... 38325 38326 38328] TEST: [    1    26    31 ... 38319 38323 38327]\n",
      "file filtering completed\n",
      "len of raw_data: 38329 train_data: 30663 test_data: 7666\n",
      "-------------------------------------------------------------------------------------------------\n",
      "cross-valid epoch: 4\n",
      "TRAIN: [    0     1     2 ... 38324 38326 38327] TEST: [    3     8    10 ... 38322 38325 38328]\n",
      "file filtering completed\n",
      "len of raw_data: 38329 train_data: 30663 test_data: 7666\n",
      "-------------------------------------------------------------------------------------------------\n",
      "cross-valid epoch: 5\n",
      "TRAIN: [    0     1     3 ... 38325 38327 38328] TEST: [    2     4     6 ... 38289 38309 38326]\n",
      "file filtering completed\n",
      "len of raw_data: 38329 train_data: 30664 test_data: 7665\n",
      "-------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import logging\n",
    "import os.path\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from Feature import Feature\n",
    "from Decoder import Decoder\n",
    "import prepare_data\n",
    "from evaluating_utils import evaluate\n",
    "\n",
    "\n",
    "logging.basicConfig(filename='5-fold-cv-process.log', \\\n",
    "    format='%(asctime)s - %(levelname)s: %(message)s', level=logging.DEBUG)\n",
    "logging.info(\"This is a info log.\")\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=10, shuffle=True)\n",
    "\n",
    "train_file = 'data/fusion_data.txt'\n",
    "raw_data = prepare_data.read_file(train_file)\n",
    "cv_epoch_num = 1\n",
    "\n",
    "for train_id, test_id in kf.split(raw_data):\n",
    "    logging.info(\"cross-valid epoch: %d\", cv_epoch_num)\n",
    "    print(\"cross-valid epoch:\", cv_epoch_num)\n",
    "\n",
    "    train_id_str = str(train_id[0:3]) + '...' + str(train_id[-3:])\n",
    "    test_id_str = str(test_id[0:3]) + '...' + str(test_id[-3:])\n",
    "    logging.info(\"TRAIN: %s, TEST: %s\", train_id_str, test_id_str)\n",
    "    print(\"TRAIN:\", train_id, \"TEST:\", test_id)\n",
    "\n",
    "    train_data, test_data = np.array(raw_data)[train_id], np.array(raw_data)[test_id]\n",
    "\n",
    "    gold_test_filepath = \"data/cv-\" + str(cv_epoch_num) + \"-filter_test.txt\"\n",
    "    prepare_data.prepare_data(test_data, gold_test_filepath)\n",
    "\n",
    "    logging.info(\"len of raw_data: %d, train_data: %d, test_data: %d\", \\\n",
    "        len(raw_data), len(train_data), len(test_data))\n",
    "    print(\"len of raw_data:\", len(raw_data), \"train_data:\", len(train_data), \"test_data:\", len(test_data))\n",
    "    logging.info(\"-------------------------------------------------------------------------------------------------\")\n",
    "    print(\"-------------------------------------------------------------------------------------------------\")\n",
    "    cv_epoch_num += 1\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start cross-validation.\n",
      "cross-valid epoch: 1\n",
      "iter 0 , finish 3.26%\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, random_state=10, shuffle=True)\n",
    "\n",
    "train_file = 'data/fusion_data.txt'\n",
    "raw_data = prepare_data.read_file(train_file)\n",
    "cv_epoch_num = 1\n",
    "\n",
    "for train_id, test_id in kf.split(raw_data):\n",
    "    print(\"Start cross-validation.\")\n",
    "    logging.info(\"Start cross-validation.\")\n",
    "    logging.info(\"cross-valid epoch: %d\", cv_epoch_num)\n",
    "    print(\"cross-valid epoch:\", cv_epoch_num)\n",
    "    \n",
    "    train_data, test_data = np.array(raw_data)[train_id], np.array(raw_data)[test_id]\n",
    "\n",
    "    train_avg(iterations=3, train_data=train_data, beam_size=16, cv_epoch_num=cv_epoch_num)\n",
    "    print(\"-------------------------------------------------------------------------------------------------\")\n",
    "    test_avg(iterations=3, test_data=test_data, beam_size=16, cv_epoch_num=cv_epoch_num)\n",
    "    print(\"-------------------------------------------------------------------------------------------------\")\n",
    "    print(\"-------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    cv_epoch_num += 1\n",
    "    # print(len(raw_data), len(train_data), len(test_data))\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluating_utils import evaluate\n",
    "\n",
    "cv_iter = 1\n",
    "beam_size = 16\n",
    "for cv_iter in range(1, 6):\n",
    "    test_filepath = 'cv-' + str(cv_iter) + '-test_seg_data/avg-test-seg-data' + '_beam-size-' + str(beam_size) + '.txt'\n",
    "    gold_test_filepath = \"data/cv-\" + str(cv_iter) + \"-filter_test.txt\"\n",
    "    word_precision, word_recall, word_fmeasure = evaluate(test_filepath, gold_test_filepath)\n",
    "    \n",
    "    logging.info(\"cv-%d-eval:\", cv_iter)\n",
    "    logging.info(\"precision: %.3f\" % word_precision)\n",
    "    logging.info(\"recall: %.3f\" % word_recall)\n",
    "    logging.info(\"F1: %.3f\" % word_fmeasure)\n",
    "    logging.info(\"-------------------------------------------------------------\")\n",
    "\n",
    "    cv_iter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6152f7bfa03653fb118ecd7c5fd279534c3a46a58fd7506dc522a094114d8e26"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('torch_cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
